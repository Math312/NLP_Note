## 1.1 研究背景
本篇文章之前出现的问题：
1. 自然语言处理系统和技术将单词视为原子单位，不考虑单词之间的相似性，仅仅在词汇表中视为一个索引。例如N-gram模型。这种方式的优点如下：
	1. 简单
	2. 鲁棒性好
	3. 使用较多数据训练出的简单模型是优于较少数据训练出的复杂模型的。
	但是也存在一些弊端：对于NLP领域用于训练的数据的量总是有一定限度的，因此，盲目靠量去提高模型的精确度是很难的。因此需要去考虑更先进的技术。
2. 之前的模型在训练数据的性能上看：都不能满足训练亿级单词的数据，单词向量的维数只能维持在50-100左右。
3. `Latent Semantic Analysis`(LSA)和`Latent Dirichlet Allocation`(LDA)也曾经被提出用于构建单词的连续表示模型，但本篇文章提到的模型在`保持单词之间的线性规律方面优于LSA`，而`LDA在大数据集上的计算成本十分昂贵`。